{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608fc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# You can install these packages using pip if not installed:\n",
    "# !pip install pandas sqlite3 pandasql\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Set working directory\n",
    "import os\n",
    "os.chdir(\"/Users/jade/Desktop/Humana/Training\")\n",
    "\n",
    "# Load datasets using pandas\n",
    "features_data = pd.read_csv(\"Afeatures.csv\")\n",
    "control_data = pd.read_csv(\"Controlpoint.csv\")\n",
    "cost_data = pd.read_csv(\"CostUt.csv\")\n",
    "demograph_data = pd.read_csv(\"Demographics.csv\")\n",
    "condition_data = pd.read_csv(\"mcondition.csv\")\n",
    "detail_data = pd.read_csv(\"mdetail.csv\")\n",
    "claims_data = pd.read_csv(\"mclaims.csv\")\n",
    "target_data = pd.read_csv(\"Tmembers.csv\")\n",
    "members_data = pd.read_csv(\"mdata.csv\")\n",
    "pharmacy_data = pd.read_csv(\"PU.csv\")\n",
    "quality_data = pd.read_csv(\"QD.csv\")\n",
    "sales_data = pd.read_csv(\"SC.csv\")\n",
    "social_data = pd.read_csv(\"Socialh.csv\")\n",
    "web_data = pd.read_csv(\"WA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0462eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_tenure(value):\n",
    "    # Check if value is a string before processing\n",
    "    if isinstance(value, str):\n",
    "        # If the format is 'X - Y YEARS', calculate the midpoint\n",
    "        if '-' in value and 'YEARS' in value:\n",
    "            numbers = re.findall(r'\\d*\\.?\\d+', value)\n",
    "            if len(numbers) == 2:  # Ensure there are two numbers for the range\n",
    "                return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        # If the format is 'X+ YEARS', take the number before the '+'\n",
    "        elif '+' in value and 'YEARS' in value:\n",
    "            numbers = re.findall(r'\\d*\\.?\\d+', value)\n",
    "            if len(numbers) == 1:  # Ensure there's at least one number\n",
    "                return float(numbers[0])\n",
    "        # If there's just 'X YEARS', return the number\n",
    "        elif 'YEARS' in value:\n",
    "            numbers = re.findall(r'\\d*\\.?\\d+', value)\n",
    "            if len(numbers) == 1:\n",
    "                return float(numbers[0])\n",
    "        # Default case for strings that don't match the expected format\n",
    "        return None\n",
    "    # If value is already a float or an int, return it as is\n",
    "    elif isinstance(value, (float, int)):\n",
    "        return value\n",
    "    # Default return for unexpected types (e.g., NaN, None)\n",
    "    return None\n",
    "\n",
    "# Applying the function to the column\n",
    "members_data['tenure_band'] = members_data['tenure_band'].apply(convert_tenure)\n",
    "members_data['disabled_ind'] = members_data['disabled_ind'].replace({'Y': 1, 'N': 0})\n",
    "members_data['dual_eligible_ind'] = members_data['dual_eligible_ind'].replace({'Y': 1, 'N': 0})\n",
    "members_data['lis_ind']= members_data['lis_ind'].replace({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4138650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "\n",
    "# Function to remove high correlation and low variance columns\n",
    "def clean_data(df, variance_threshold=0.01, correlation_threshold=0.9):\n",
    "    # Step 1: Remove variables with low variance\n",
    "    selector = VarianceThreshold(threshold=variance_threshold)\n",
    "    df_reduced = selector.fit_transform(df)\n",
    "    \n",
    "    # Convert back to DataFrame to handle column names\n",
    "    columns_kept = df.columns[selector.get_support()]\n",
    "    df_filtered = pd.DataFrame(df_reduced, columns=columns_kept)\n",
    "\n",
    "    # Step 2: Remove highly correlated variables\n",
    "    corr_matrix = df_filtered.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > correlation_threshold)]\n",
    "    \n",
    "    # Drop highly correlated columns\n",
    "    df_cleaned = df_filtered.drop(columns=to_drop)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Assuming your data is already loaded in these variables\n",
    "# Apply the cleaning process to each dataset\n",
    "members_data_cleaned = clean_data(members_data)\n",
    "pharmacy_data_cleaned = clean_data(pharmacy_data)\n",
    "social_data_cleaned = clean_data(social_data)\n",
    "web_data_cleaned = clean_data(web_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_cleaned = pd.merge(members_data_cleaned, pharmacy_data_cleaned, on='id', how='inner')\n",
    "merged_data_cleaned = pd.merge(merged_data, social_data_cleaned, on='id', how='inner')\n",
    "merged_data_cleaned = pd.merge(merged_data, web_data_cleaned, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3d563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(members_data, pharmacy_data, on='id', how='inner')\n",
    "merged_data = pd.merge(merged_data, social_data, on='id', how='inner')\n",
    "merged_data = pd.merge(merged_data, web_data, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a5712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_target=target_data.drop(['product_type','calendar_year','plan_category'],axis=1)\n",
    "merged_data = pd.merge(merged_data, new_target, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'features_data': features_data,\n",
    "    'control_data': control_data,\n",
    "    'cost_data': cost_data,\n",
    "    'demograph_data': demograph_data,\n",
    "    'condition_data': condition_data,\n",
    "    'detail_data': detail_data,\n",
    "    'claims_data': claims_data,\n",
    "    'target_data': target_data,\n",
    "    'members_data': members_data,\n",
    "    'pharmacy_data': pharmacy_data,\n",
    "    'quality_data': quality_data,\n",
    "    'sales_data': sales_data,\n",
    "    'social_data': social_data,\n",
    "    'web_data': web_data\n",
    "}\n",
    "\n",
    "# Check number of rows for each dataset\n",
    "for name, data in datasets.items():\n",
    "    print(f\"{name} has {len(data)} rows\")\n",
    "    print(f\"{name} has {data.info()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1271d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#condition_data\n",
    "#cliams_data\n",
    "#quality_data\n",
    "features_data \n",
    "control_data \n",
    "cost_data \n",
    "demograph_data #deleted two columns \n",
    "#detail_data  \n",
    "target_data \n",
    "members_data \n",
    "pharmacy_data \n",
    "#sales_data \n",
    "social_data \n",
    "web_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d625c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(merged_data, features_data, on='id', how='inner')\n",
    "merged_data = pd.merge(merged_data, control_data, on='id', how='inner')\n",
    "merged_data = pd.merge(merged_data, cost_data, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6617ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c27c9e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "demograph_data = pd.read_csv(\"Demographics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0803013",
   "metadata": {},
   "outputs": [],
   "source": [
    "demograph_data=demograph_data.drop(columns=['lang_spoken_cd', 'rucc_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1f53f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(merged_data, demograph_data, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f53472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming merged_data is already available\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "X = merged_data.drop(columns=['target_column'])  # Features\n",
    "y = merged_data['target_column']  # Target\n",
    "\n",
    "# Step 2: Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Step 3: Dimensionality reduction (optional) - reduce to 10 components\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_imputed)\n",
    "\n",
    "# Step 4: Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train Random Forest with fewer estimators and use parallel processing\n",
    "rf_model = RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate performance\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", rf_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54703b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
